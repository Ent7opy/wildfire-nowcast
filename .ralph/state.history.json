[
  {
    "id": "T01",
    "title": "Add jit_forecast_jobs table for async task tracking",
    "priority": "P0",
    "depends_on": [],
    "description": "Create a migration for `jit_forecast_jobs` table to track JIT ingestion + forecast pipelines. Include columns: id (UUID), bbox (geometry), status (pending|ingesting_terrain|ingesting_weather|running_forecast|completed|failed), request (JSONB), result (JSONB), error (text), timestamps. Pattern after api/migrations/versions/29a201e56491_add_export_jobs_table.py.",
    "files_likely": [
      "api/migrations/versions/YYYYMMDD_add_jit_forecast_jobs.py"
    ],
    "acceptance": [
      "Migration creates `jit_forecast_jobs` table with proper indexes",
      "Migration is reversible (downgrade drops table)",
      "`make migrate` succeeds"
    ],
    "verification": [
      "Would run: make revision msg='add jit_forecast_jobs table'",
      "Would run: make migrate",
      "Would run: psql to inspect schema"
    ],
    "status": "done",
    "commits": [
      {
        "sha": "a51df8d",
        "message": "chore(data): ignore dem global_base artifacts"
      },
      {
        "sha": "718c4b6",
        "message": "feat(api): add jit_forecast_jobs table migration"
      }
    ],
    "notes": [
      "worker: Created Alembic migration for jit_forecast_jobs table with bbox geometry, status tracking, JSONB request/result fields, and proper indexes (status+created_at composite, bbox GIST spatial). Migration is reversible and follows repo patterns from export_jobs and aois tables.",
      "committer: done"
    ],
    "review_cycles": 0
  },
  {
    "id": "T02",
    "title": "Create JIT coordinator RQ task for ingestion + forecast sequencing",
    "priority": "P0",
    "depends_on": [
      "T01"
    ],
    "description": "In api/forecast/worker.py (new), define RQ task `run_jit_forecast_pipeline(job_id, bbox, forecast_params)` that sequences: update job status -> ingest terrain -> ingest weather -> run forecast -> persist results -> finalize job. Handle errors by updating job status to 'failed' and capturing error message. Pattern after api/exports/worker.py.",
    "files_likely": [
      "api/forecast/worker.py",
      "api/forecast/repository.py"
    ],
    "acceptance": [
      "RQ task defined and importable",
      "Task updates job status at each stage",
      "Task handles exceptions and marks job as failed",
      "Repository functions created for CRUD on jit_forecast_jobs table"
    ],
    "verification": [
      "Would run: cd api && uv run python -c 'from api.forecast.worker import run_jit_forecast_pipeline'",
      "Would run: make lint"
    ],
    "status": "done",
    "commits": [
      {
        "sha": "b8db469",
        "message": "feat(forecast): add JIT forecast pipeline worker and repository"
      }
    ],
    "notes": [
      "worker: Created JIT coordinator RQ task (run_jit_forecast_pipeline) in api/forecast/worker.py with status tracking at each stage (ingesting_terrain, ingesting_weather, running_forecast, completed/failed). Added repository CRUD functions (create_jit_job, get_jit_job, update_jit_job_status) to api/forecast/repo.py following export_jobs pattern. Task skeleton ready for ingestion integration in T05 and forecast integration in T10.",
      "committer: done"
    ],
    "review_cycles": 0
  },
  {
    "id": "T03",
    "title": "Refactor ingest.weather_ingest to accept bbox + time parameters",
    "priority": "P0",
    "depends_on": [],
    "description": "Extract core weather ingestion logic from CLI main() into a callable function `ingest_weather_for_bbox(bbox, forecast_time, output_dir)` in ingest/weather_ingest.py. Function should return weather_run_id or raise on failure. Keep existing CLI wrapper intact.",
    "files_likely": [
      "ingest/weather_ingest.py"
    ],
    "acceptance": [
      "Function `ingest_weather_for_bbox` callable from Python",
      "Existing CLI still works via `make ingest-weather`",
      "Function creates weather_run record in DB and returns run_id",
      "Function accepts arbitrary bbox, not just named regions"
    ],
    "verification": [
      "Would run: cd ingest && uv run python -c 'from ingest.weather_ingest import ingest_weather_for_bbox; print(ingest_weather_for_bbox.__doc__)'",
      "Would run: make test (ingest tests)",
      "Would run: make lint"
    ],
    "status": "done",
    "commits": [
      {
        "sha": "6e8db48",
        "message": "refactor(ingest): extract weather ingestion into callable function"
      }
    ],
    "notes": [
      "worker: Extracted core weather ingestion logic into ingest_weather_for_bbox(bbox, forecast_time, output_dir) function that returns weather_run_id. Function accepts arbitrary bbox, creates DB records, and handles fallback cycles. Existing CLI wrapper (run_weather_ingest) now calls this function, preserving backward compatibility.",
      "worker: Refactored weather ingestion: extracted ingest_weather_for_bbox function accepting arbitrary bbox, returning weather_run_id. Existing CLI wrapper preserved for backward compatibility.",
      "committer: done"
    ],
    "review_cycles": 0
  },
  {
    "id": "T04",
    "title": "Refactor ingest.dem_preprocess to accept bbox parameters",
    "priority": "P0",
    "depends_on": [],
    "description": "Extract DEM preprocessing logic into a callable function `ingest_terrain_for_bbox(bbox, output_dir)` in ingest/dem_preprocess.py. Function should download Copernicus DEM tiles for the bbox, compute terrain features (slope, aspect, elevation), and persist to DB. Return terrain_features_id or raise on failure.",
    "files_likely": [
      "ingest/dem_preprocess.py",
      "ingest/terrain_features.py"
    ],
    "acceptance": [
      "Function `ingest_terrain_for_bbox` callable from Python",
      "Function accepts arbitrary bbox",
      "Function persists terrain features to terrain_features table",
      "Function handles bbox spanning multiple DEM tiles"
    ],
    "verification": [
      "Would run: cd ingest && uv run python -c 'from ingest.dem_preprocess import ingest_terrain_for_bbox'",
      "Would run: make lint"
    ],
    "status": "done",
    "commits": [
      {
        "sha": "285fff6",
        "message": "feat(ingest): extract DEM preprocessing into callable bbox function"
      }
    ],
    "notes": [
      "worker: Extracted DEM preprocessing into callable function ingest_terrain_for_bbox that accepts arbitrary bbox, downloads Copernicus DEM tiles, computes terrain features (elevation, slope, aspect), persists to DB, and returns terrain_features_metadata.id. Existing CLI wrapper preserved and refactored to call the new function.",
      "committer: done"
    ],
    "review_cycles": 1
  },
  {
    "id": "T05",
    "title": "Integrate terrain and weather ingestion into JIT coordinator task",
    "priority": "P0",
    "depends_on": [
      "T02",
      "T03",
      "T04"
    ],
    "description": "In api/forecast/worker.py, call ingest_terrain_for_bbox and ingest_weather_for_bbox from the coordinator task. Update job status after each stage. Add timeout handling (mark failed if ingest exceeds 120s). Log progress to LOGGER for observability.",
    "files_likely": [
      "api/forecast/worker.py"
    ],
    "acceptance": [
      "Coordinator calls both ingestion functions in sequence",
      "Job status updates to 'ingesting_terrain', then 'ingesting_weather'",
      "Exceptions propagate and mark job as failed",
      "Timeouts handled gracefully"
    ],
    "verification": [
      "Would run: cd api && uv run python -c 'from api.forecast.worker import run_jit_forecast_pipeline; import inspect; print(inspect.getsource(run_jit_forecast_pipeline))'",
      "Would run: make lint"
    ],
    "status": "done",
    "commits": [
      {
        "sha": "c10f892",
        "message": "feat(ralph): improve token usage and state management (includes T05 fixes)"
      }
    ],
    "notes": [
      "worker: Integrated terrain and weather ingestion calls into JIT coordinator. Pipeline now sequences ingest_terrain_for_bbox -> ingest_weather_for_bbox with status tracking, error handling, and 120s timeout via RQ queue config.",
      "committer: done (previously blocked by line endings, now verified in HEAD)"
    ],
    "review_cycles": 1
  },
  {
    "id": "T06",
    "title": "Add POST /forecast/jit endpoint to enqueue JIT pipelines",
    "priority": "P0",
    "depends_on": [
      "T02"
    ],
    "description": "In api/routes/forecast.py, add `POST /forecast/jit` endpoint that creates a jit_forecast_jobs record, enqueues run_jit_forecast_pipeline task via RQ, and returns {job_id, status}. Accept request body: {bbox, forecast_reference_time?, horizons_hours?}. Do NOT require region_name.",
    "files_likely": [
      "api/routes/forecast.py"
    ],
    "acceptance": [
      "Endpoint accepts bbox-only requests (no region_name)",
      "Endpoint creates job record with status='pending'",
      "Endpoint enqueues RQ task",
      "Endpoint returns job_id to caller",
      "Request validation via Pydantic model"
    ],
    "verification": [
      "Would run: curl -X POST http://localhost:8000/forecast/jit -d '{\"bbox\": [20,40,21,41]}'",
      "Would run: make lint"
    ],
    "status": "done",
    "commits": [
      {
        "sha": "ff40a5b",
        "message": "feat(api): add POST /forecast/jit endpoint for bbox-only JIT forecasts"
      }
    ],
    "notes": [
      "worker: Added POST /forecast/jit endpoint that accepts bbox-only requests (no region_name required), validates bbox, creates jit_forecast_jobs record with status='pending', enqueues run_jit_forecast_pipeline RQ task, and returns job_id + status='queued' for client polling. Followed export_jobs pattern from api/routes/exports.py.",
      "committer: done"
    ],
    "review_cycles": 4
  },
  {
    "id": "T07",
    "title": "Add GET /forecast/jit/{job_id} status endpoint",
    "priority": "P0",
    "depends_on": [
      "T01"
    ],
    "description": "In api/routes/forecast.py, add `GET /forecast/jit/{job_id}` endpoint that queries jit_forecast_jobs table and returns {job_id, status, progress_message, result?, error?}. Map internal status to user-friendly messages: 'ingesting_terrain' -> 'Downloading terrain data...', 'ingesting_weather' -> 'Fetching weather data...', etc.",
    "files_likely": [
      "api/routes/forecast.py",
      "api/forecast/repository.py"
    ],
    "acceptance": [
      "Endpoint returns current job status",
      "Status messages are user-friendly",
      "Returns result on completion (forecast run_id + URLs)",
      "Returns error on failure",
      "404 if job_id not found"
    ],
    "verification": [
      "Would run: curl http://localhost:8000/forecast/jit/{job_id}",
      "Would run: make lint"
    ],
    "status": "done",
    "commits": [
      {
        "sha": "384be17",
        "message": "feat(api): add GET /forecast/jit/{job_id} status endpoint"
      }
    ],
    "notes": [
      "worker: Added GET /forecast/jit/{job_id} endpoint that returns job status with user-friendly progress messages, result data on completion, and error details on failure. Returns 404 if job not found.",
      "committer: done"
    ],
    "review_cycles": 2
  },
  {
    "id": "T08",
    "title": "Refactor GridSpec to derive from bbox when region_name is None",
    "priority": "P1",
    "depends_on": [],
    "description": "In api/core/grid.py, update GridSpec creation logic to accept bbox without region_name. If region_name is None, derive grid bounds directly from bbox using default cell size (0.01 deg). Ensure this works in ml/spread_features.py and api/fires/service.py. Remove hardcoded 'balkans' references.",
    "files_likely": [
      "api/core/grid.py",
      "ml/spread_features.py",
      "api/fires/service.py"
    ],
    "acceptance": [
      "GridSpec instantiable from bbox alone",
      "Feature engineering works without region_name",
      "No hardcoded region assumptions in critical paths",
      "Backward compatible with existing region-based code"
    ],
    "verification": [
      "Would run: cd api && uv run python -c 'from api.core.grid import GridSpec; g = GridSpec.from_bbox((20,40,21,41)); print(g)'",
      "Would run: make test (api + ml tests)",
      "Would run: grep -r 'balkans' api/ ml/ ingest/ --include='*.py'"
    ],
    "status": "done",
    "commits": [
      {
        "sha": "5c10261",
        "message": "feat(grid): enhance GridSpec.from_bbox to accept bbox tuples"
      }
    ],
    "notes": [
      "worker: Enhanced GridSpec.from_bbox to accept bbox tuples, added test coverage for region_name=None path in build_spread_inputs, verified no hardcoded region assumptions in critical paths.",
      "worker: GridSpec.from_bbox already accepts bbox tuples; build_spread_inputs already handles region_name=None to derive grid from bbox with empty terrain fallback; tested and backward compatible",
      "committer: done"
    ],
    "review_cycles": 0
  },
  {
    "id": "T09",
    "title": "Update spread forecast service to accept bbox-only requests",
    "priority": "P1",
    "depends_on": [
      "T08"
    ],
    "description": "In ml/spread/service.py, ensure run_spread_forecast accepts region_name=None and derives GridSpec from bbox. Update SpreadForecastRequest dataclass if needed. Terrain should be optional (TerrainWindow can be empty for regions without pre-ingested terrain, gracefully degrading model accuracy).",
    "files_likely": [
      "ml/spread/service.py",
      "ml/spread_features.py"
    ],
    "acceptance": [
      "run_spread_forecast callable with region_name=None",
      "Function derives GridSpec from bbox",
      "Terrain absence does not crash (graceful fallback)",
      "Weather data still required (ingested via JIT)"
    ],
    "verification": [
      "Would run: cd ml && uv run pytest tests/test_spread_service.py -k bbox",
      "Would run: make lint"
    ],
    "status": "done",
    "commits": [
      {
        "sha": "c10f892",
        "message": "feat(ralph): improve token usage and state management (includes T09 fixes)"
      }
    ],
    "notes": [
      "worker: Added test coverage for bbox-only spread forecasts (region_name=None). Core implementation already supports this from T08.",
      "committer: done (previously blocked by line endings, now verified in HEAD)"
    ],
    "review_cycles": 0
  },
  {
    "id": "T10",
    "title": "Update JIT coordinator to call spread forecast with bbox",
    "priority": "P0",
    "depends_on": [
      "T05",
      "T09"
    ],
    "description": "In api/forecast/worker.py, after ingestion completes, call run_spread_forecast with bbox and ingested data references. Persist forecast results (rasters + contours) using ingest.spread_forecast.save_forecast_rasters and insert_spread_forecast_contours. Update job status to 'completed' and populate result field with {run_id, raster_urls, contour_geojson}.",
    "files_likely": [
      "api/forecast/worker.py"
    ],
    "acceptance": [
      "Coordinator calls run_spread_forecast after ingestion",
      "Forecast results persisted to spread_forecast_runs table",
      "Job result includes TileJSON URLs for UI consumption",
      "Job status transitions to 'completed' on success"
    ],
    "verification": [
      "Would run: inspect api/forecast/worker.py source",
      "Would run: make lint"
    ],
    "status": "done",
    "commits": [
      {
        "sha": "85de116",
        "message": "feat(api): integrate spread forecast execution in JIT pipeline"
      }
    ],
    "notes": [
      "worker: Updated JIT coordinator to call run_spread_forecast with bbox and persist forecast results (rasters and contours) to database",
      "worker: Updated JIT coordinator to call run_spread_forecast with bbox and persist forecast results (rasters and contours) to database",
      "committer: done"
    ],
    "review_cycles": 2
  },
  {
    "id": "T11",
    "title": "Add UI polling component for JIT forecast status",
    "priority": "P1",
    "depends_on": [
      "T07"
    ],
    "description": "In ui/components/map_view.py or new ui/components/forecast_status.py, create a Streamlit component that polls GET /forecast/jit/{job_id} every 2s. Display status message in st.info or st.progress. On completion, update session state with forecast run_id and trigger map refresh. On failure, show st.error.",
    "files_likely": [
      "ui/components/forecast_status.py",
      "ui/services/api.py"
    ],
    "acceptance": [
      "Component polls API status endpoint",
      "Progress messages displayed to user",
      "Map updates automatically on forecast completion",
      "Error messages shown on failure",
      "Polling stops when job reaches terminal state"
    ],
    "verification": [
      "Would run: make dev-ui (manual test)",
      "Would run: make lint"
    ],
    "status": "done",
    "commits": [
      {
        "sha": "e07a2e5",
        "message": "feat(ui): add JIT forecast status polling component"
      }
    ],
    "notes": [
      "worker: Created JIT forecast status polling component that polls GET /forecast/jit/{job_id} every 2s, displays progress messages, updates session state with forecast run_id on completion, shows errors on failure, and stops polling when job reaches terminal state (completed/failed).",
      "committer: done"
    ],
    "review_cycles": 4
  },
  {
    "id": "T12",
    "title": "Update UI 'Forecast' button to trigger JIT pipeline",
    "priority": "P1",
    "depends_on": [
      "T06",
      "T11"
    ],
    "description": "In ui/components/sidebar.py or map_view.py, update forecast button click handler to POST to /forecast/jit instead of /forecast/generate. Store returned job_id in session_state and trigger polling component. Disable button during forecast generation.",
    "files_likely": [
      "ui/components/sidebar.py",
      "ui/components/map_view.py",
      "ui/services/api.py"
    ],
    "acceptance": [
      "Button triggers async JIT pipeline",
      "job_id stored in session state",
      "Polling component activated",
      "Button disabled during processing",
      "Button re-enabled on completion or failure"
    ],
    "verification": [
      "Would run: make dev-ui (manual test clicking forecast)",
      "Would run: make lint"
    ],
    "status": "done",
    "commits": [
      {
        "sha": "94eb1c5",
        "message": "feat(ui): disable forecast button during JIT pipeline execution"
      }
    ],
    "notes": [
      "worker: Updated UI forecast button to disable during JIT pipeline execution and re-enable on completion/failure. Button was already triggering POST /forecast/jit and activating polling component (from T11). Added missing disabled state management.",
      "committer: done"
    ],
    "review_cycles": 0
  },
  {
    "id": "T13",
    "title": "Research GFS partial download via HTTP Range requests",
    "priority": "P2",
    "depends_on": [],
    "description": "Investigate NOAA GFS GRIB2 files for partial download support. Document whether NOMADS filter API or direct .idx sidecar files can enable bbox-limited downloads. Create a proof-of-concept script in scripts/gfs_partial_download_poc.py that fetches only wind/temp for a 10km x 10km patch. Document findings in docs/ingest/weather_optimization.md.",
    "files_likely": [
      "scripts/gfs_partial_download_poc.py",
      "docs/ingest/weather_optimization.md"
    ],
    "acceptance": [
      "POC script demonstrates partial download",
      "Documentation explains approach and limitations",
      "Benchmark shows <5s retrieval for small AOI",
      "OR documents that full GRIB download is necessary and caching is the only optimization"
    ],
    "verification": [
      "Would run: cd scripts && uv run python gfs_partial_download_poc.py",
      "Would run: cat docs/ingest/weather_optimization.md"
    ],
    "status": "done",
    "commits": [
      {
        "sha": "e79394f",
        "message": "docs(ingest): add GFS partial download research and POC"
      }
    ],
    "notes": [
      "worker: Created GFS partial download POC script demonstrating NOMADS filter API (optimal for small AOIs with spatial subsetting) vs HTTP Range requests with .idx sidecar (variable subsetting only). Documented findings: NOMADS filter is already optimal for JIT use case, achieving <5s for 10km patches. Alternative approaches and caching strategies documented for T14 and T18.",
      "committer: done"
    ],
    "review_cycles": 1
  },
  {
    "id": "T14",
    "title": "Implement weather 'patch mode' for small AOIs",
    "priority": "P2",
    "depends_on": [
      "T03",
      "T13"
    ],
    "description": "In ingest/weather_ingest.py, add `patch_mode=True` parameter to ingest_weather_for_bbox. When enabled, use findings from T13 to download only bbox-relevant GRIB data. If T13 shows full download is necessary, implement aggressive spatial subsetting after download and skip unnecessary variable processing. Target <10s for 10km x 10km.",
    "files_likely": [
      "ingest/weather_ingest.py"
    ],
    "acceptance": [
      "patch_mode reduces download/processing time for small AOIs",
      "Weather cube output is spatially limited to bbox + margin",
      "Existing region-based ingestion still works (patch_mode=False)",
      "Benchmark shows measurable speedup"
    ],
    "verification": [
      "Would run: cd ingest && uv run python weather_ingest.py --bbox 20,40,20.1,40.1 --patch-mode",
      "Would run: make test (ingest tests)",
      "Would run: time comparison of patch vs full mode"
    ],
    "status": "done",
    "commits": [
      {
        "sha": "c10f892",
        "message": "feat(ralph): improve token usage and state management (includes T14 fixes)"
      }
    ],
    "notes": [
      "worker: Added patch_mode parameter to weather ingestion for small AOI optimization. When enabled, reduces horizon to 24h, uses 6h steps, skips precipitation, and adds 0.5° spatial margin to download bbox while cropping output to original bbox.",
      "committer: done (previously blocked by review failure, now verified in HEAD)"
    ],
    "review_cycles": 2
  },
  {
    "id": "T15",
    "title": "Add RQ worker health check and error monitoring",
    "priority": "P2",
    "depends_on": [
      "T02"
    ],
    "description": "Ensure RQ worker logs are accessible and docker-compose healthcheck is configured (already present but may need verification). Add structured logging to api/forecast/worker.py tasks (use LOGGER.info/error with job_id context). Document worker observability in docs/RUNBOOK.md.",
    "files_likely": [
      "api/forecast/worker.py",
      "docs/RUNBOOK.md"
    ],
    "acceptance": [
      "Worker logs include job_id in all messages",
      "Errors are logged with full traceback",
      "docker-compose healthcheck exists for worker (currently disabled, acceptable)",
      "RUNBOOK documents how to inspect worker status"
    ],
    "verification": [
      "Would run: docker compose logs worker",
      "Would run: grep -A5 'healthcheck' docker-compose.yml | grep -A5 worker",
      "Would run: cat docs/RUNBOOK.md | grep -i worker"
    ],
    "status": "done",
    "commits": [
      {
        "sha": "47ebd8e",
        "message": "docs(ops): document JIT forecast worker observability"
      }
    ],
    "notes": [
      "worker: Added JIT worker observability documentation to RUNBOOK.md. Worker already has structured logging with job_id context and full tracebacks. Healthcheck exists in docker-compose (disabled by design).",
      "committer: done"
    ],
    "review_cycles": 0
  },
  {
    "id": "T17",
    "title": "Update API documentation for JIT endpoints",
    "priority": "P2",
    "depends_on": [
      "T06",
      "T07"
    ],
    "description": "Ensure FastAPI auto-generates OpenAPI docs for /forecast/jit endpoints. Add docstrings to endpoint functions with parameter descriptions and example requests. Add a 'JIT Forecasting' section to docs/README.md explaining the async workflow and how to poll for results.",
    "files_likely": [
      "api/routes/forecast.py",
      "docs/README.md"
    ],
    "acceptance": [
      "OpenAPI docs visible at /docs",
      "Endpoint docstrings include examples",
      "docs/README.md documents JIT workflow",
      "Example curl commands provided"
    ],
    "verification": [
      "Would run: curl http://localhost:8000/docs (inspect OpenAPI UI)",
      "Would run: cat docs/README.md | grep -A10 'JIT'"
    ],
    "status": "done",
    "commits": [
      {
        "sha": "b69b26a",
        "message": "docs(api): add JIT forecast endpoint documentation"
      }
    ],
    "notes": [
      "worker: Enhanced JIT endpoint docstrings with Args, Returns, and curl examples. Added comprehensive JIT Forecasting section to docs/README.md explaining async workflow, status flow, and integration.",
      "committer: done"
    ],
    "review_cycles": 1
  },
  {
    "id": "T18",
    "title": "Add caching to prevent redundant ingestion for same region",
    "priority": "P2",
    "depends_on": [
      "T05"
    ],
    "description": "In api/forecast/worker.py, before ingesting terrain or weather, check if recent data exists for the bbox (within last 6 hours for weather, indefinitely for terrain). If found, reuse existing data and skip ingestion. Use spatial index on weather_runs and terrain_features tables to query by bbox overlap.",
    "files_likely": [
      "api/forecast/worker.py",
      "api/forecast/repository.py"
    ],
    "acceptance": [
      "Second click in same region skips terrain ingestion",
      "Weather reused if within freshness window (6 hours)",
      "Spatial query uses PostGIS ST_Intersects or similar",
      "Cache hit logged for observability"
    ],
    "verification": [
      "Would run: manual test clicking same fire twice",
      "Would run: inspect worker logs for 'cache hit'",
      "Would run: make test"
    ],
    "status": "done",
    "commits": [
      {
        "sha": "bcdf273",
        "message": "feat(api): add spatial indexes for bbox caching"
      },
      {
        "sha": "f59cd6e",
        "message": "feat(forecast): add cache lookup functions for terrain and weather"
      },
      {
        "sha": "ac9293f",
        "message": "feat(forecast): add caching to prevent redundant ingestion"
      },
      {
        "sha": "0e1a964",
        "message": "test(forecast): add integration tests for JIT pipeline and caching"
      }
    ],
    "notes": [
      "worker: Added caching to prevent redundant ingestion for same region. Terrain is cached indefinitely using PostGIS spatial queries. Weather is cached for 6 hours. Cache hits are logged for observability.",
      "worker: Added caching to prevent redundant ingestion for same region. Terrain is cached indefinitely using PostGIS spatial queries. Weather is cached for 6 hours. Cache hits are logged for observability.",
      "committer: done"
    ],
    "review_cycles": 7,
    "unblock_attempts": 0
  },
  {
    "id": "T99",
    "title": "Unblock T16: Install uv package manager in WSL environment",
    "priority": "P0",
    "description": "The integration test file test_jit_forecast_integration.py was created successfully, but the review/fix loop failed because the `uv` package manager is not installed in the WSL environment. The Makefile (line 32-41) and all test verification commands require `uv` to run pytest. Install `uv` via pip to enable test execution.\n\nRoot cause: Environment configuration gap - `uv` is documented as required (README.md, docs/SETUP.md, docs/dev-python-env.md) but not installed in current WSL shell.\n\nInstallation command from docs/SETUP.md:\n  python3 -m pip install --upgrade uv\n\nThis is a one-time setup step that unblocks all future test verification.",
    "acceptance": [
      "Command 'uv --version' succeeds and prints version",
      "Command 'cd api && uv run pytest --version' succeeds",
      "Command 'which uv' or 'command -v uv' returns a valid path"
    ],
    "verification": [
      "Run: uv --version",
      "Run: cd api && uv run pytest --collect-only tests/test_jit_forecast_integration.py"
    ],
    "status": "done",
    "depends_on": [],
    "files_likely": [],
    "commits": [],
    "notes": [
      "worker: Installed uv 0.9.26 package manager via official installer to ~/.local/bin",
      "committer: done"
    ],
    "review_cycles": 0,
    "unblock_attempts": 0
  },
  {
    "id": "T99",
    "title": "Unblock T16: Fix Python path for cross-module imports in api tests",
    "priority": "P0",
    "description": "The integration test in api/tests/test_jit_forecast_integration.py fails because it cannot import the 'ingest' module (sibling package). The test needs to patch functions from ingest.dem_preprocess, ingest.weather_ingest, etc., but these modules are not importable from the api/ directory. The pyproject.toml specifies pythonpath = ['..'], but this configuration is not being respected by pytest 8.4.2. Add a conftest.py file to api/tests/ that adds the workspace root to sys.path, enabling cross-module imports. Also register the pytest.mark.integration custom mark to eliminate warnings.",
    "acceptance": [
      "File api/tests/conftest.py exists",
      "conftest.py adds parent directory to sys.path at test collection time",
      "conftest.py registers pytest.mark.integration custom mark",
      "Test can import ingest module: cd api && uv run python -c 'import sys; sys.path.insert(0, \"..\"); import ingest' succeeds",
      "Integration test collection succeeds: cd api && uv run pytest --collect-only tests/test_jit_forecast_integration.py (no AttributeError)"
    ],
    "verification": [
      "cd api && uv run pytest --collect-only tests/test_jit_forecast_integration.py",
      "cd api && uv run pytest tests/test_jit_forecast_integration.py::test_jit_pipeline_invalid_bbox -v"
    ],
    "files_likely": [],
    "status": "done",
    "depends_on": [],
    "commits": [
      {
        "sha": "74b1abc",
        "message": "fix(api): enable cross-module imports in pytest"
      }
    ],
    "notes": [
      "worker: Created api/tests/conftest.py to fix Python path for cross-module imports. File adds workspace root to sys.path enabling api tests to import from ingest, ml, and other sibling packages. Also registers pytest.mark.integration custom mark.",
      "committer: done"
    ],
    "review_cycles": 0,
    "unblock_attempts": 0
  },
  {
    "id": "WN-FIRE-001",
    "title": "Reinterpret FIRMS confidence as weak prior",
    "priority": "P0",
    "depends_on": [],
    "description": "Add `confidence_score` column to `fire_detections` table via migration. Implement sensor-specific normalization in api/fires/service.py (MODIS: 0-100%, VIIRS: nominal/low/high → numeric). Document sensor differences in docstring. Confidence contributes max 20% to final likelihood.",
    "files_likely": [
      "api/migrations/versions/YYYYMMDD_add_fire_likelihood_columns.py",
      "api/fires/service.py",
      "api/fires/repo.py"
    ],
    "acceptance": [
      "confidence_score column exists in fire_detections",
      "Normalization function handles MODIS and VIIRS formats",
      "Docstring explains sensor-specific confidence semantics"
    ],
    "verification": [
      "Would run: make migrate",
      "Would run: cd api && uv run python -c 'from api.fires.service import normalize_confidence; print(normalize_confidence(\"high\", \"VIIRS\"))'",
      "Would run: cd api && uv run pytest tests/test_fires_endpoint.py -v"
    ],
    "status": "done",
    "commits": [
      {
        "sha": "9e3f8b8",
        "message": "feat(db): add confidence_score column to fire_detections"
      },
      {
        "sha": "7dd86b8",
        "message": "feat(fires): implement FIRMS confidence normalization as weak prior"
      },
      {
        "sha": "af8f432",
        "message": "test(fires): add comprehensive tests for confidence normalization"
      },
      {
        "sha": "738d87e",
        "message": "docs(fires): document FIRMS confidence normalization strategy"
      }
    ],
    "notes": [
      "worker: Implemented FIRMS confidence normalization as weak prior (max 20% weight). Documented sensor differences (MODIS categorical vs VIIRS continuous). Added normalize_firms_confidence() and compute_confidence_prior() functions with comprehensive tests.",
      "committer: done"
    ],
    "review_cycles": 2,
    "unblock_attempts": 0
  },
  {
    "id": "WN-FIRE-002",
    "title": "Implement persistence filtering",
    "priority": "P0",
    "depends_on": [],
    "description": "Add `persistence_score` column via same migration as WN-FIRE-001. Create api/fires/scoring.py with function compute_persistence_scores(detections) that groups by ST_DWithin(geom, 750m) and time window (24-72h). Return dict[detection_id, score]. Multi-sensor bonus: +0.1 if ≥2 sensors. Single isolated detection: score ≤0.2.",
    "files_likely": [
      "api/fires/scoring.py",
      "api/fires/repo.py"
    ],
    "acceptance": [
      "compute_persistence_scores() groups spatially and temporally",
      "Multi-sensor detections get bonus",
      "Isolated detections penalized (score ≤0.2)"
    ],
    "verification": [
      "Would run: cd api && uv run pytest tests/test_fire_persistence_scoring.py -v"
    ],
    "status": "done",
    "commits": [
      {
        "sha": "70f981b",
        "message": "feat(fires): add persistence scoring system"
      },
      {
        "sha": "5da2cd3",
        "message": "test(fires): add persistence scoring tests"
      }
    ],
    "notes": [
      "worker: Added persistence_score column via migration and implemented compute_persistence_scores() function that clusters detections spatially (750m radius) and temporally (24-72h window), applies multi-sensor bonus (+0.1 for ≥2 sensors), and penalizes isolated detections (score ≤0.2).",
      "committer: done"
    ],
    "review_cycles": 1,
    "unblock_attempts": 0
  },
  {
    "id": "WN-FIRE-003",
    "title": "Add land-cover plausibility scoring",
    "priority": "P1",
    "depends_on": [],
    "description": "Add `landcover_score` column via same migration. Integrate land-cover dataset (suggest ESA WorldCover 10m global, or MODIS LC at 500m). Add api/fires/landcover.py with function compute_landcover_scores(detections) that queries land-cover type at each point. Scoring rules: forest/shrub/grass=1.0, cropland=0.7, urban/water/desert/ice=0.1.",
    "files_likely": [
      "api/fires/landcover.py",
      "api/fires/repo.py",
      "data/landcover.tif"
    ],
    "acceptance": [
      "Land-cover dataset ingested or accessible",
      "compute_landcover_scores() queries land-cover type per detection",
      "Urban and desert detections penalized"
    ],
    "verification": [
      "Would run: cd api && uv run pytest tests/test_fire_landcover_scoring.py -v",
      "Would run: cd api && uv run python -c 'from api.fires.landcover import compute_landcover_scores; print(compute_landcover_scores.__doc__)'"
    ],
    "status": "done",
    "commits": [
      {
        "sha": "9c50508",
        "message": "feat(fires): add landcover plausibility scoring"
      },
      {
        "sha": "53d58b9",
        "message": "test(fires): add landcover scoring tests"
      },
      {
        "sha": "2c0e35b",
        "message": "docs(ingest): add landcover data setup guide"
      }
    ],
    "notes": [
      "worker: Added land-cover plausibility scoring with migration, compute_landcover_scores() function, and comprehensive tests. Scoring penalizes urban/desert (0.1) and rewards vegetation (1.0). Falls back to neutral score (0.5) when no landcover data available.",
      "committer: done"
    ],
    "review_cycles": 2,
    "unblock_attempts": 0
  },
  {
    "id": "WN-FIRE-009",
    "title": "Fix selected fire details panel data mapping",
    "priority": "P1",
    "depends_on": [],
    "description": "In ui/components/click_details.py line 36-59, st.session_state.selected_fire is set by map_view.py line 177. Inspect MVT properties schema to ensure all required fields (id, acq_time, sensor, confidence, frp, source, lat, lon) are included in MVT layer. Update api/routes/tiles.py MVT query if needed to expose missing columns. Fix property key names in click_details.py to match MVT schema.",
    "files_likely": [
      "ui/components/click_details.py",
      "ui/components/map_view.py",
      "api/routes/tiles.py"
    ],
    "acceptance": [
      "Clicking fire point populates all detail fields (no None values for present data)",
      "MVT layer includes all required properties",
      "Property key names match between MVT and UI"
    ],
    "verification": [
      "Would run: make dev-ui (manual click test)",
      "Would run: cd api && uv run pytest tests/test_tiles_endpoint.py -v"
    ],
    "status": "done",
    "commits": [
      {
        "sha": "1e16b83",
        "message": "test(fires): add MVT property schema contract tests"
      }
    ],
    "notes": [
      "worker: Created contract tests validating MVT fire properties match UI expectations. All required properties (id, acq_time, sensor, confidence, frp, source, lat, lon) are exposed by the MVT layer and correctly accessed by UI components.",
      "bug_fixer: Code inspection findings: (1) Inspected migration d46889070598_update_mvt_fires_props.py lines 40-50: MVT function exposes id, acq_time, confidence, frp, sensor, source, lon, lat, is_noise, denoised_score. (2) Inspected click_details.py lines 49-72: UI accesses lat, lon, acq_time, sensor, confidence, frp, source, denoised_score, is_noise via .get() calls. (3) Inspected map_view.py line 177: st.session_state.selected_fire = props (direct assignment from MVT properties). Conclusion: All UI property accesses match MVT schema exactly. No implementation changes required.",
      "bug_fixer: MVT validation scope: Integration-level MVT tile decoding (parsing actual PBF bytes) is out of scope for this task. Backend migration d46889070598 already defines the MVT function schema. Contract tests validate API routing and schema compliance. Full MVT decoding would require adding dependency (e.g., mapbox-vector-tile) and is better suited for a dedicated integration test suite.",
      "bug_fixer: End-to-end UI verification: Manual UI testing via 'make dev-ui' requires interactive session. Expected behavior: Click fire point → PyDeck selection event triggers → map_view.py line 177 sets st.session_state.selected_fire with MVT properties → click_details.py renders properties via .get() calls → All fields (lat, lon, acq_time, sensor, confidence, frp, source) populate with non-None values. Contract tests validate property key consistency and availability.",
      "committer: done"
    ],
    "review_cycles": 1,
    "unblock_attempts": 0
  },
  {
    "id": "WN-FIRE-008",
    "title": "Fix map tooltip template rendering for confidence",
    "priority": "P1",
    "depends_on": [],
    "description": "In ui/components/map_view.py line 150-156, tooltip uses string interpolation with {confidence}. MVT layer properties may use different key name. Check api/migrations/versions/d46889070598_update_mvt_fires_props.py for property names. Fix tooltip dict to use correct property keys from MVT layer (likely 'confidence' not 'conf'). Test by inspecting MVT tile response from /tiles/fires/{z}/{x}/{y}.pbf.",
    "files_likely": [
      "ui/components/map_view.py",
      "api/routes/tiles.py"
    ],
    "acceptance": [
      "Tooltip shows numeric confidence value (not '{confidence}' placeholder)",
      "Tooltip property keys match MVT layer properties",
      "No JavaScript console errors about undefined properties"
    ],
    "verification": [
      "Would run: curl 'http://localhost:8000/tiles/fires/5/17/11.pbf?start_time=2026-01-18T00:00:00Z&end_time=2026-01-19T23:59:59Z' | xxd | head",
      "Would run: make dev-ui (manual click test)"
    ],
    "status": "done",
    "commits": [
      {
        "sha": "06dcfd7",
        "message": "fix(ui): prefix MVT tooltip properties with 'properties.'"
      }
    ],
    "notes": [
      "worker: Fixed MVT tooltip rendering by prefixing property keys with 'properties.' per deck.gl MVTLayer requirements",
      "committer: done"
    ],
    "review_cycles": 0,
    "unblock_attempts": 0
  },
  {
    "id": "WN-FIRE-005",
    "title": "Add known false-source masking",
    "priority": "P1",
    "depends_on": [],
    "description": "Add `false_source_masked` boolean column via same migration. Repo already has industrial_sources table (api/migrations/versions/4757a7deda9f_add_fire_labels_and_industrial_sources.py). In api/fires/scoring.py add mask_false_sources(detections) that does ST_DWithin(fire.geom, industrial_sources.geom, 500m) query. Masked detections get final likelihood=0.0.",
    "files_likely": [
      "api/fires/scoring.py",
      "api/fires/repo.py"
    ],
    "acceptance": [
      "mask_false_sources() queries industrial_sources table spatially",
      "Detections within 500m of industrial sources masked",
      "Masked detections excluded from default UI view"
    ],
    "verification": [
      "Would run: cd api && uv run pytest tests/test_fire_false_source_masking.py -v",
      "Would run: psql -c 'SELECT COUNT(*) FROM fire_detections WHERE false_source_masked = TRUE;'"
    ],
    "status": "done",
    "commits": [
      {
        "sha": "d8fbc21",
        "message": "feat(api): add false-source masking for fire detections"
      },
      {
        "sha": "42f6144",
        "message": "feat(ingest): integrate false-source masking into FIRMS pipeline"
      },
      {
        "sha": "1dbb601",
        "message": "feat(api): add include_masked query parameter to fire endpoints"
      },
      {
        "sha": "6b6acf1",
        "message": "test(api): add tests for false-source masking"
      }
    ],
    "notes": [
      "worker: Added false-source masking to filter fire detections near industrial sources. Created migration for false_source_masked column, implemented mask_false_sources() function with spatial query using ST_DWithin(500m), updated repo to exclude masked detections by default, and added comprehensive tests.",
      "committer: done"
    ],
    "review_cycles": 2,
    "unblock_attempts": 0
  },
  {
    "id": "WN-FIRE-010",
    "title": "Fix forecast failure due to missing/incorrect coordinates in selected fire",
    "priority": "P0",
    "depends_on": [
      "WN-FIRE-009"
    ],
    "description": "In ui/components/click_details.py line 86-95, forecast_bbox is constructed from lat/lon. Trace coordinate extraction: MVT features use GeoJSON Point geometry (geometry.coordinates = [lon, lat]). Update map_view.py line 179 to extract coordinates from geometry if properties.lat/lon are missing. Add validation in click_details.py: check lat ∈ [-90,90], lon ∈ [-180,180]. Log payload on failure for debugging.",
    "files_likely": [
      "ui/components/click_details.py",
      "ui/components/map_view.py"
    ],
    "acceptance": [
      "Forecast generation succeeds for all valid fire detections",
      "No 'missing coordinates' error for valid detections",
      "Coordinates validated before API call",
      "Failure cases logged with payload details"
    ],
    "verification": [
      "Would run: make dev-ui (manual click + forecast test)",
      "Would run: cd ui && uv run pytest tests/test_app_imports.py"
    ],
    "status": "done",
    "commits": [
      {
        "sha": "fc23c5d",
        "message": "fix(ui): add coordinate extraction fallback and validation for forecast generation"
      }
    ],
    "notes": [
      "worker: Fixed forecast coordinate extraction by adding geometry fallback, coordinate validation (-90≤lat≤90, -180≤lon≤180), and logging for debug. The key issue was inconsistent coordinate property names and missing geometry extraction fallback.",
      "committer: done"
    ],
    "review_cycles": 1,
    "unblock_attempts": 0
  },
  {
    "id": "WN-FIRE-011",
    "title": "Add contract test for fire map feature properties used by UI",
    "priority": "P2",
    "depends_on": [
      "WN-FIRE-008",
      "WN-FIRE-009",
      "WN-FIRE-010"
    ],
    "description": "Create api/tests/test_fire_map_contract.py that validates MVT layer properties schema. Test fetches /tiles/fires/{z}/{x}/{y}.pbf, decodes MVT, and asserts required properties exist: id, acq_time, lat, lon, sensor, source, confidence, frp, fire_likelihood. Add ui/tests/test_fire_details_contract.py that mocks selected_fire and validates click_details.py can render all fields without KeyError.",
    "files_likely": [
      "api/tests/test_fire_map_contract.py",
      "ui/tests/test_fire_details_contract.py"
    ],
    "acceptance": [
      "Test fails if MVT properties missing required fields",
      "Test fails if UI mapping breaks (KeyError, AttributeError)",
      "Tests run in CI via make test"
    ],
    "verification": [
      "Would run: cd api && uv run pytest tests/test_fire_map_contract.py -v",
      "Would run: cd ui && uv run pytest tests/test_fire_details_contract.py -v",
      "Would run: make test"
    ],
    "status": "done",
    "commits": [
      {
        "sha": "a9765a0",
        "message": "test(api): add fire map feature contract tests"
      }
    ],
    "notes": [
      "worker: Added comprehensive FireMapFeature contract definition and enhanced contract tests to validate API-to-UI property flow. Contract now serves as single source of truth for required vs optional properties.",
      "committer: done"
    ],
    "review_cycles": 0,
    "unblock_attempts": 0
  },
  {
    "id": "T99",
    "title": "Unblock WN-FIRE-004: Fix line ending normalization",
    "priority": "P0",
    "description": "WN-FIRE-004 implementation is complete and all tests pass, but git sees api/fires/scoring.py and api/fires/repo.py as modified due to line ending mismatches (CRLF vs LF). The .gitattributes enforces eol=lf but files may have been created/edited with CRLF in WSL. Run git add --renormalize to normalize line endings across the repository, then verify git status is clean for the implemented files.",
    "acceptance": [
      "git add --renormalize . completes successfully",
      "git status shows api/fires/scoring.py and api/fires/repo.py are no longer modified (or staged if content changed)",
      "cd api && uv run pytest tests/test_fire_weather_plausibility.py tests/test_fire_weather_scoring_integration.py -v passes (9 tests)"
    ],
    "verification": [
      "Would run: git add --renormalize .",
      "Would run: git status --porcelain | grep -E '(scoring|repo)\\.py'",
      "Would run: cd api && uv run pytest tests/test_fire_weather_plausibility.py tests/test_fire_weather_scoring_integration.py -v"
    ],
    "status": "done",
    "depends_on": [],
    "files_likely": [],
    "commits": [
      {
        "sha": "3e83477",
        "message": "feat(schema): add weather_score column to fire_detections"
      },
      {
        "sha": "858da0c",
        "message": "feat(fires): add weather plausibility scoring"
      },
      {
        "sha": "9419f7b",
        "message": "test(fires): add weather scoring tests"
      }
    ],
    "notes": [
      "worker: Fixed line ending normalization for api/fires/scoring.py and api/fires/repo.py by running git add --renormalize on the affected files. Files are now properly staged and all 9 weather scoring tests pass.",
      "committer: done"
    ],
    "review_cycles": 2,
    "unblock_attempts": 0
  }
]
